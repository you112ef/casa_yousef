#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sky CASA - AI Sperm Analysis System
ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ

Support for:
- Static Images (PNG, JPG, JPEG)
- Video Files (MP4, AVI, MOV)
- Real-time CASA metrics calculation
- WHO standards compliance checking

Requirements:
pip install ultralytics opencv-python numpy pandas scipy filterpy deep-sort-realtime
"""

import cv2
import numpy as np
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
import os
import json
import sqlite3
from datetime import datetime
from utils.casa_metrics import CASACalculator
from utils.who_standards import WHOStandards

class SpermAnalyzer:
    def __init__(self, model_path="models/sperm-analyzer-v1.pt", db_path="../../database.db"):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ©
        
        Args:
            model_path: Ù…Ø³Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
            db_path: Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        
        self.model_path = model_path
        self.db_path = db_path
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.load_model()
        
        # ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØªØ¨Ø¹
        self.tracker = DeepSort(max_age=30, n_init=3)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ø³Ø¨ CASA metrics
        self.casa_calculator = CASACalculator()
        
        # Ù…Ø¹Ø§ÙŠÙŠØ± WHO
        self.who_standards = WHOStandards()
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØªØ¨Ø¹
        self.tracks_data = {}
        self.analysis_results = {}
        
        print("ğŸ§¬ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù„Ù„ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
    
    def load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            if os.path.exists(self.model_path):
                self.model = YOLO(self.model_path)
                print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {self.model_path}")
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…ØªÙˆÙØ±
                self.model = YOLO('yolov8n.pt')
                print("âš ï¸  Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ - ÙŠÙÙ†ØµØ­ Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØµØµ")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            raise
    
    def analyze_image(self, image_path, patient_id, save_results=True):
        """
        ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ©
        
        Args:
            image_path: Ù…Ø³Ø§Ø± Ø§Ù„ØµÙˆØ±Ø©
            patient_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø±ÙŠØ¶
            save_results: Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            
        Returns:
            dict: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        """
        print(f"ğŸ”¬ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {image_path}")
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©: {image_path}")
        
        # ÙƒØ´Ù Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ©
        results = self.model(image)[0]
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        detections = []
        if results.boxes is not None:
            for box in results.boxes:
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                confidence = box.conf[0].cpu().numpy()
                detections.append({
                    'bbox': [int(x1), int(y1), int(x2), int(y2)],
                    'confidence': float(confidence),
                    'center': [(x1+x2)/2, (y1+y2)/2]
                })
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        analysis_result = {
            'patient_id': patient_id,
            'image_path': image_path,
            'analysis_type': 'image',
            'timestamp': datetime.now().isoformat(),
            'total_count': len(detections),
            'detections': detections,
            'ai_confidence': np.mean([d['confidence'] for d in detections]) if detections else 0,
            'concentration_estimation': self.estimate_concentration_from_image(len(detections)),
            'who_compliance': self.who_standards.check_count_compliance(len(detections))
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø­Ù„Ù„Ø©
        analyzed_image = self.draw_detections(image.copy(), detections)
        analyzed_path = self.save_analyzed_image(analyzed_image, image_path, 'analyzed')
        analysis_result['analyzed_image_path'] = analyzed_path
        
        # Ø­ÙØ¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        heatmap = self.generate_heatmap(image.shape, detections)
        heatmap_path = self.save_analyzed_image(heatmap, image_path, 'heatmap')
        analysis_result['heatmap_path'] = heatmap_path
        
        if save_results:
            self.save_to_database(analysis_result)
        
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(detections)} Ø­ÙŠÙˆØ§Ù† Ù…Ù†ÙˆÙŠ")
        return analysis_result
    
    def analyze_video(self, video_path, patient_id, duration_seconds=10, save_results=True):
        """
        ØªØ­Ù„ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ© Ù…Ø¹ Ø­Ø³Ø§Ø¨ CASA metrics
        
        Args:
            video_path: Ù…Ø³Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
            patient_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø±ÙŠØ¶
            duration_seconds: Ù…Ø¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
            save_results: Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            
        Returns:
            dict: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ CASA metrics
        """
        print(f"ğŸ¬ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙØªØ­ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(fps * duration_seconds)
        
        self.tracks_data = {}
        frame_count = 0
        processed_frames = []
        
        print(f"ğŸ“¹ Ù…Ø¹Ø§Ù„Ø¬Ø© {total_frames} Ø¥Ø·Ø§Ø± Ø¨Ø³Ø±Ø¹Ø© {fps} Ø¥Ø·Ø§Ø±/Ø«Ø§Ù†ÙŠØ©")
        
        while cap.isOpened() and frame_count < total_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ÙƒØ´Ù Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ©
            results = self.model(frame)[0]
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØªØ¨Ø¹
            detections = []
            if results.boxes is not None:
                for box in results.boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    w, h = x2-x1, y2-y1
                    conf = box.conf[0].cpu().numpy()
                    
                    detections.append(([int(x1), int(y1), int(w), int(h)], conf, 'sperm'))
            
            # Ø§Ù„ØªØªØ¨Ø¹
            tracks = self.tracker.update_tracks(detections, frame=frame)
            timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)
            
            # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØªØ¨Ø¹
            for track in tracks:
                if track.is_confirmed() and track.track_id:
                    tid = track.track_id
                    x1, y1, x2, y2 = track.to_ltrb()
                    center_x, center_y = (x1+x2)/2, (y1+y2)/2
                    
                    if tid not in self.tracks_data:
                        self.tracks_data[tid] = []
                    
                    self.tracks_data[tid].append({
                        'timestamp': timestamp_ms,
                        'position': (center_x, center_y),
                        'bbox': [x1, y1, x2, y2],
                        'frame': frame_count
                    })
            
            # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø±
            annotated_frame = self.draw_tracks(frame.copy(), tracks)
            processed_frames.append(annotated_frame)
            
            frame_count += 1
            
            if frame_count % 30 == 0:
                print(f"â³ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {frame_count}/{total_frames} Ø¥Ø·Ø§Ø±...")
        
        cap.release()
        
        # Ø­Ø³Ø§Ø¨ CASA metrics
        casa_metrics = self.calculate_casa_metrics()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙƒØ©
        motility_analysis = self.analyze_motility()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªØ§Ø¦Ø¬ Ø´Ø§Ù…Ù„Ø©
        analysis_result = {
            'patient_id': patient_id,
            'video_path': video_path,
            'analysis_type': 'video',
            'timestamp': datetime.now().isoformat(),
            'duration_seconds': duration_seconds,
            'fps': fps,
            'total_frames': frame_count,
            'total_tracks': len(self.tracks_data),
            'valid_tracks': len([t for t in self.tracks_data.values() if len(t) >= 10]),
            'casa_metrics': casa_metrics,
            'motility_analysis': motility_analysis,
            'who_compliance': self.who_standards.check_full_compliance(casa_metrics),
            'ai_confidence': casa_metrics.get('detection_confidence', 0)
        }
        
        # Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø­Ù„Ù„
        analyzed_video_path = self.save_analyzed_video(processed_frames, video_path, fps)
        analysis_result['analyzed_video_path'] = analyzed_video_path
        
        if save_results:
            self.save_to_database(analysis_result)
        
        print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(self.tracks_data)} Ù…Ø³Ø§Ø± Ø­ÙŠÙˆØ§Ù† Ù…Ù†ÙˆÙŠ")
        return analysis_result
    
    def calculate_casa_metrics(self):
        """Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± CASA Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØªØ¨Ø¹"""
        if not self.tracks_data:
            return {}
        
        print("ğŸ“Š Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± CASA...")
        
        all_vcl, all_vsl, all_vap = [], [], []
        all_lin, all_str, all_wob = [], [], []
        all_alh, all_bcf = [], []
        
        for track_id, positions in self.tracks_data.items():
            if len(positions) < 10:  # ØªØ­ØªØ§Ø¬ 10 Ù†Ù‚Ø§Ø· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
                continue
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ ÙˆØ§Ù„Ø£ÙˆÙ‚Ø§Øª
            times = [p['timestamp'] for p in positions]
            coords = [p['position'] for p in positions]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø§Øª
            vcl, vsl, vap = self.casa_calculator.calculate_velocities(coords, times)
            lin, str_val, wob = self.casa_calculator.calculate_linearity_params(vcl, vsl, vap)
            alh = self.casa_calculator.calculate_amplitude_lateral_head(coords)
            bcf = self.casa_calculator.calculate_beat_frequency(coords, times)
            
            if vcl > 0:  # Ù‚ÙŠÙ… ØµØ§Ù„Ø­Ø© ÙÙ‚Ø·
                all_vcl.append(vcl)
                all_vsl.append(vsl)
                all_vap.append(vap)
                all_lin.append(lin)
                all_str.append(str_val)
                all_wob.append(wob)
                all_alh.append(alh)
                all_bcf.append(bcf)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
        casa_metrics = {}
        if all_vcl:
            casa_metrics = {
                'vcl_mean': np.mean(all_vcl),
                'vsl_mean': np.mean(all_vsl),
                'vap_mean': np.mean(all_vap),
                'lin_mean': np.mean(all_lin),
                'str_mean': np.mean(all_str),
                'wob_mean': np.mean(all_wob),
                'alh_mean': np.mean(all_alh),
                'bcf_mean': np.mean(all_bcf),
                'vcl_std': np.std(all_vcl),
                'vsl_std': np.std(all_vsl),
                'detection_confidence': 0.9  # Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³Ø§Øª
            }
        
        return casa_metrics
    
    def analyze_motility(self):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø­Ø±ÙƒØ© Ø­Ø³Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± WHO"""
        if not self.tracks_data:
            return {}
        
        rapid_progressive = 0  # Grade A
        slow_progressive = 0   # Grade B
        non_progressive = 0    # Grade C
        immotile = 0          # Grade D
        
        for track_id, positions in self.tracks_data.items():
            if len(positions) < 5:
                immotile += 1
                continue
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©
            coords = [p['position'] for p in positions]
            times = [p['timestamp'] for p in positions]
            
            vcl, vsl, _ = self.casa_calculator.calculate_velocities(coords, times)
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ø±ÙƒØ© Ø­Ø³Ø¨ WHO
            if vsl >= 25:  # Ø³Ø±Ø¹Ø© Ø®Ø·ÙŠØ© â‰¥ 25 Î¼m/s
                rapid_progressive += 1
            elif vsl >= 5:  # Ø³Ø±Ø¹Ø© Ø®Ø·ÙŠØ© 5-24 Î¼m/s
                slow_progressive += 1
            elif vcl >= 5:  # Ø­Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù†
                non_progressive += 1
            else:
                immotile += 1
        
        total = len(self.tracks_data)
        
        return {
            'rapid_progressive_count': rapid_progressive,
            'slow_progressive_count': slow_progressive,
            'non_progressive_count': non_progressive,
            'immotile_count': immotile,
            'rapid_progressive_percent': (rapid_progressive/total)*100 if total else 0,
            'slow_progressive_percent': (slow_progressive/total)*100 if total else 0,
            'non_progressive_percent': (non_progressive/total)*100 if total else 0,
            'immotile_percent': (immotile/total)*100 if total else 0,
            'total_progressive_percent': ((rapid_progressive+slow_progressive)/total)*100 if total else 0,
            'total_motile_percent': ((total-immotile)/total)*100 if total else 0
        }
    
    def draw_detections(self, image, detections):
        """Ø±Ø³Ù… Ø§Ù„ÙƒØ´ÙˆÙØ§Øª Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©"""
        for detection in detections:
            bbox = detection['bbox']
            conf = detection['confidence']
            
            # Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹
            cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)
            
            # ÙƒØªØ§Ø¨Ø© Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
            cv2.putText(image, f'{conf:.2f}', 
                       (bbox[0], bbox[1]-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        return image
    
    def draw_tracks(self, frame, tracks):
        """Ø±Ø³Ù… Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªØªØ¨Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø±"""
        for track in tracks:
            if track.is_confirmed() and track.track_id:
                x1, y1, x2, y2 = map(int, track.to_ltrb())
                
                # Ø±Ø³Ù… Ù…Ø±Ø¨Ø¹ Ø§Ù„ØªØªØ¨Ø¹
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ÙƒØªØ§Ø¨Ø© Ù…Ø¹Ø±Ù Ø§Ù„ØªØªØ¨Ø¹
                cv2.putText(frame, f'ID: {track.track_id}', 
                           (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                
                # Ø±Ø³Ù… Ø§Ù„Ù…Ø³Ø§Ø±
                if track.track_id in self.tracks_data and len(self.tracks_data[track.track_id]) > 1:
                    points = [p['position'] for p in self.tracks_data[track.track_id][-10:]]
                    points = [(int(p[0]), int(p[1])) for p in points]
                    
                    for i in range(1, len(points)):
                        cv2.line(frame, points[i-1], points[i], (255, 0, 0), 2)
        
        return frame
    
    def generate_heatmap(self, image_shape, detections):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ù„ÙƒØ´ÙˆÙØ§Øª"""
        heatmap = np.zeros((image_shape[0], image_shape[1]), dtype=np.float32)
        
        for detection in detections:
            center = detection['center']
            x, y = int(center[0]), int(center[1])
            
            # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø·Ø© Ø­Ø±Ø§Ø±ÙŠØ©
            cv2.circle(heatmap, (x, y), 20, 1.0, -1)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªÙ…ÙˆÙŠÙ‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ£Ø«ÙŠØ± Ø­Ø±Ø§Ø±ÙŠ Ù†Ø§Ø¹Ù…
        heatmap = cv2.GaussianBlur(heatmap, (41, 41), 0)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø®Ø±ÙŠØ·Ø© Ø£Ù„ÙˆØ§Ù†
        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)
        
        return heatmap_colored
    
    def save_analyzed_image(self, image, original_path, suffix):
        """Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø­Ù„Ù„Ø©"""
        filename = os.path.basename(original_path)
        name, ext = os.path.splitext(filename)
        new_filename = f"{name}_{suffix}{ext}"
        
        output_path = os.path.join("outputs", new_filename)
        os.makedirs("outputs", exist_ok=True)
        
        cv2.imwrite(output_path, image)
        return output_path
    
    def save_analyzed_video(self, frames, original_path, fps):
        """Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø­Ù„Ù„"""
        filename = os.path.basename(original_path)
        name, ext = os.path.splitext(filename)
        new_filename = f"{name}_analyzed{ext}"
        
        output_path = os.path.join("outputs", new_filename)
        os.makedirs("outputs", exist_ok=True)
        
        if frames:
            height, width, _ = frames[0].shape
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            for frame in frames:
                out.write(frame)
            
            out.release()
        
        return output_path
    
    def estimate_concentration_from_image(self, count):
        """ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªØ±ÙƒÙŠØ² Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ÙˆÙŠØ© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©"""
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ· - ÙŠØ­ØªØ§Ø¬ Ù…Ø¹Ø§ÙŠØ±Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø¹ÙŠÙ†Ø© ÙˆØ§Ù„ØªÙƒØ¨ÙŠØ±
        estimated_concentration = count * 2.5  # Ù…Ù„ÙŠÙˆÙ†/Ù…Ù„
        return estimated_concentration
    
    def save_to_database(self, results):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¯Ø±Ø§Ø¬
            data = self.prepare_database_data(results)
            
            # Ø¥Ø¯Ø±Ø§Ø¬ ÙÙŠ Ø¬Ø¯ÙˆÙ„ semen_analysis
            cursor.execute("""
                INSERT INTO semen_analysis (
                    patient_id, test_date, ai_analysis_performed, ai_model_version,
                    ai_confidence_score, vcl_um_s, vsl_um_s, lin_percent,
                    original_image_path, original_video_path, analyzed_image_path,
                    analyzed_video_path, heatmap_image_path, total_tracks_detected,
                    valid_tracks_count, tracking_duration_seconds, frames_analyzed,
                    detection_accuracy_percent, comments, qc_status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, data)
            
            conn.commit()
            conn.close()
            
            print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    def prepare_database_data(self, results):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        casa_metrics = results.get('casa_metrics', {})
        motility = results.get('motility_analysis', {})
        
        return (
            results['patient_id'],
            datetime.now(),
            True,  # ai_analysis_performed
            'YOLOv8-DeepSORT-v1.0',
            results.get('ai_confidence', 0),
            casa_metrics.get('vcl_mean', 0),
            casa_metrics.get('vsl_mean', 0),
            casa_metrics.get('lin_mean', 0),
            results.get('image_path'),
            results.get('video_path'),
            results.get('analyzed_image_path'),
            results.get('analyzed_video_path'),
            results.get('heatmap_path'),
            results.get('total_tracks', 0),
            results.get('valid_tracks', 0),
            results.get('duration_seconds', 0),
            results.get('total_frames', 0),
            casa_metrics.get('detection_confidence', 0) * 100,
            f"AI Analysis - Total: {results.get('total_count', 0)} detected",
            'Approved'
        )

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    print("ğŸ§¬ Sky CASA - AI Sperm Analysis System")
    print("=" * 50)
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù„Ù„
    analyzer = SpermAnalyzer()
    
    # Ù…Ø«Ø§Ù„ Ù„ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø©
    # results = analyzer.analyze_image("sample_image.jpg", patient_id=1)
    
    # Ù…Ø«Ø§Ù„ Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆ
    # results = analyzer.analyze_video("sample_video.mp4", patient_id=1, duration_seconds=15)
    
    print("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")